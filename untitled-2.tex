\documentclass[11pt, onecolumn, a4paper]{report}

\usepackage{biblatex}
\addbibresource{references.bib}
\bibliographystyle{unsrt}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[francais]{babel}
\usepackage{setspace}
\usepackage{soul}
\usepackage{ulem}
%usepackage{color}
\usepackage{url}
\usepackage[top=1.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}
\usepackage{layout}
\usepackage{lmodern}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{array}
\usepackage{eurosym}
%\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{makeidx}
\usepackage[absolute]{textpos}
\usepackage[cc]{titlepic}
\usepackage{booktabs}
\usepackage[table,xcdraw]{xcolor}

%\usepackage[landscape]{geometry}
\usepackage{listings}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{subcaption}


\usepackage[capposition=top]{floatrow}


\onehalfspacing
\title{Master thesis}
\author{patrick guerin}
\date{October 2018}

\begin{document}

\maketitle

\chapter*{Abstract}
Abstract goes here
 
\chapter*{Dedication}
To mum and dad
 
\chapter*{Declaration}
I declare that..
 
\chapter*{Acknowledgements}
I want to thank...

\tableofcontents


\chapter{Introduction}

\section*{Context}
\addcontentsline{toc}{section}{Context}

read introduction of deep learning book for ideas
% pb with the order, I speak about montecarlo too early, first write bellman equality equation then introduce tabular method with monte carlo then td learning
Neural networks were inspired by the Nobel prize winning work of Hubel and
Wiesel on the primary visual cortex of cats (Hubel & Wiesel 1962). Their seminal
experiments showed that neuronal networks where organized in hierarchical layers
of cells for processing visual stimulus



during the last years renewal of neural network  sur le devant de la scene

form feature designing to architecture designing

from SIFT (Lowe, 1999), and HOG (Dalal & Triggs, 2005), to AlexNet (Krizhevsky
et al., 2012), VGGNet (Simonyan & Zisserman, 2014), GoogleNet (Szegedy et al., 2015), and
ResNet (He et al., 2016a)

speech recognition (Hinton et al., 2012), image recognition (LeCun et al., 1998;
Krizhevsky et al., 2012) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2015; Wu
et al., 2016).

Recent papers proposed several principles in the design of architecture but it still require a lot of expertise and remains particurlarly time-consuming (mal dit) 

underline relevance of the subject, historic intro
 Hinton et al 2012

presentation of of neural architecture search


non convex property hard to understand from theoretical point of view

Contributions

explain what I add to the field witrh my thesis
what do i study


\section*{Contribution}
\addcontentsline{toc}{section}{Contribution}

This masterâ€™s thesis introduces a number of contributions to different aspects of neural architecture search.

In the first chapter,

In the second chapter

In the last chapter

Overall,


\chapter{Neural Architecture Search with Reinforcement Learning}

\section{Introduction}

\section{Introduction to Reinforcement learning}

% limitation and scope p24
When a child is learning to behave in society, he may first show a behaviour differing from would be accepted in society. When it happens, he is told to correct its attitude, and if he were to obstinately repeat its faulty behaviour, he will end up being scolded. Conversely, the child is encouraged (rewarded) when he adopts the right attitude in a given situation (state). In other words, the child learn by the interaction with its environment, using the feedback he receive to improve its attitude.\\

The field of reinforcement learning is centered on this very idea of learning from interaction. In reinforcement learning, we aims to use algorithms in order to automatically learn how to interact with a specified environment over a sequence of discrete time steps. The learner, called agent, takes actions to maximize a numerical reward signal $R$. The agent does not know at first which actions yield the most rewards in a given situation, but must discover it by trying them. In a lot of cases, an action affects not only the immediate reward but also the subsequent rewards, since an action taken at time $t$ can modify the \textit{state} of the agent at time $t+1$.\\

% As we can see, the reinforcement learning methods are designed for situations in which the reward is observable -if our agent learn to play chess, we can signal him that he won or lost the game- and does not apply to situations for which the reward is not observable.


\textbf{Finite Markov decision process}\\

The environment is typically formulated as a finite Markov Decision Process. \\
A finite Markov decision process is a set of 4 elements $(S,A,P_{a},R_{a})$, where

\begin{itemize}

\item $S$ is a finite set of states,
\item$A$ is a finite set of actions 
\item$P(S_{t+1}=s'|S_t=s,A_t=a)$ is the probability that taking the action $a$ in state $s$ at time $t$ will lead to state $s'$ at time $t+1$. 
\item $R_{a}(s,s')$ is the expected reward at time $t+1$ after transitioning from state $S_t=s$ to state $S_{t+1}=s'$, due to the action $A_t=a$. 
\end{itemize}


\begin{figure}[H]
\includegraphics[scale=1]{MDP.JPG}
\caption{Schema\cite{1} of the interaction between the agent and the environment. At step $t$,the agent takes the action $A_t$ given the state $S_t$.Following this action the agent is now in the state $S_{t+1}$ and receive the reward $R_{t+1}$.}
\end{figure}

The problem is to find a function $\pi$, which, given a state $s$, outputs the optimal action $\pi(s)$  This function is called \textit{policy}. what is actually optimal depends of the application, however we generally want to maximize the weighted sum of the future rewards. \\

\textbf{\Large About the scope of our reinforcement learning overview}



In some case, the optimal policy may be stochastic (in poker for instance), hence the policy may be defined as a mapping from the states to the probabilities of selecting each possible action.\\

% speak about the two pb in rl prediction: predict the action-value function given a policy , control: derive the optimal policy given the action value function
%  montecarlo method

\textbf{Markov decision process and reinforcement learning}\\

%% also speak about expected return p 45 if necessary (value of a state instead of the value of an action)

The problem of reinforcement learning can be seen as the optimal control of incompletely-known Markov decision processes. Indeed, the main difference between classical dynamic programming methods and reinforcement learning is that the latter do not require a prior knowledge of the Markov decision process transition probabilities $P_{a}(s,s')$.\\
 
Since the transition probabilities are not known, it is not possible to derive analytically an optimal, exact policy. Because of this, the problem of estimating the value of a given action in a given state is central in reinforcement learning.\\

To present this field, we will first introduce the \textit{Dynamic programming} method,which can be used to derive the optimal policy for a Markov decision process when the transition probabilities are known. Secondly, we will explore several reinforcement learning approaches to deal with the case where the transition probabilities are not known \textit{a priori}.

\subsection{Dynamic programming and Markov decision process}

Dynamic programming is a method of mathematical optimization and programming proposed in the 1950s by Richard Bellman. 
The method consists to approach a complex dynamic problem by dividing it into a sequence of $T$ decisions and $T$ states, where $T$ is the number of time steps. The key idea is use a \textbf{\textit{value function} $v(s)$} mapping a state $S_t=s$ to its value for the agent. An alternative approach is to define an \textbf{\textit{action-value function} $q(s,a)$} who yields the value of the action $a$ given the state $s$. The notion of value remains to be defined. \\

If $R_1,R_2,...,R_T$ are the rewards received after the time step $t$, the return $G_t$ (or cumulative discounted reward) following time t is

\begin{center}
$$G_t = \sum^{\infty}_{t=0} {\gamma^t R_{a_t} (s_t, s_{t+1})}$$ with
$a_t = \pi(s_t)$,
\end{center}
Where \gamma is a discount factor expressing the preference for short-term rewards compared to long-term rewards. If $\gamma$=0, the agent does not take into account futur rewards, while setting $\gamma=1$ equally weights all the rewards so that the agent  strives to reach high rewards on the long-term.\\

The value of taking an action $A_t=a$ given a state $S_t=s$ under a policy $\pi$ is 
% when time look a the fact that several optimal policies can exist

$$q_\pi(s,a) = \mathop{\mathbb{E_\pi}}[G_t | S_t=s,A_t=a] = \mathop{\mathbb{E}} \Bigg [\sum^{\infty}_{k=0} {\gamma^{k} R_{t+1+k}} \Big | S_t=s,A_t=a \Bigg ]$$

Where $\mathop{\mathbb{E}}_{\pi}[\cdot]$ is the expected value of a random variable given that the agent follows the policy $\pi$.

$q_\pi(s,a)$ is the \textit{action-value function} for policy $\pi$. $q_\pi(s,a)$ maps every tuples $(s,a)$ to its expected return.\\

Similarly,the value of a state $S_t=s$ is 

$$v_\pi(s) = \mathop{\mathbb{E_\pi}}[G_t | S_t=s] = \mathop{\mathbb{E}} \Bigg [\sum^{\infty}_{k=0} {\gamma^{k} R_{t+1+k}} \Big | S_t=s \Bigg ] \; ,\text{ for all } s \in S.$$ 

We can observe that the value-function $v_\pi(s)$ is the expectation of $q_\pi(s,a)$ over all potential actions $a$ in state $s$.\\

\textbf{Bellman equation}\\

An important result in reinforcement learning shows that the value of an action $q_\pi(s, a)$ depends on the expected immediate reward $r$ and on the expected weighted sum of the future rewards.Indeed $q_\pi(s, a)$ can be rewritten to unveil its recursive nature:

\begin{align}
q_\pi(s,a) & = \mathop{\mathbb{E_\pi}}[G_t | S_t=s,A_t=a] \nonumber \\
& = \mathop{\mathbb{E}} \Bigg [\sum^{\infty}_{k=0} {\gamma^{k} R_{t+1+k}} \Bigg ] \nonumber \\
& = \mathop{\mathbb{E_\pi}} \Bigg [r_{t+1}+\gamma\sum^{\infty}_{k=0} {\gamma^{k} R_{t+2+k}} \Bigg | S_t=s,A_t=a \Bigg ] \nonumber\\
& = \sum_{s'}\sum_{r}p(s',r|s,a) \Bigg [r + \gamma\mathop{\mathbb{E_\pi}} \Big [G_{t+1} | S_{t+1}=s' \Big] \Bigg] \nonumber \\
& = \sum_{s'}\sum_{r}p(s',r|s,a) \Bigg [r + \gamma \sum_{a'} \pi(s',a')q_\pi(s',a') \Bigg ]
\end{align}

Where $p(s',r|s,a)$ is the probability of being in state $s'$ and receiving a reward $r$ at step $t+1$, given that the agent execute the action $a$ in state $s$ at step $t$. We can see that for each double.

Equation $(1)$ is the \textit{Bellman equation} for $q_\pi$. Intuitively, it is natural to think that the value of an action in a given state should depend of the future rewards received following this action.\\

A analogous result can be obtained if the problem is posed in terms of finding the optimal value-func $v(s)$:

$$v_\pi(s)=\sum_{a}\pi(a|s)\sum_{s'}\sum_{r}p(s',r|s,a) \Bigg [r + \gamma v_\pi(s') \Bigg ]  \; ,\text{ for all } s \in S.$$ 

\\

\textbf{Optimal policies}\\\\

An \textit{optimal policy} $q_{*}$ is  such that $q_{\pi_*}(s,a) \ge q_{\pi}(s,a)$ for all $s \in S$, $a \in A(s)$ and for all policies $\pi$. \\

They may be several optimal policies, however they all have the same action-value function, called \textit{optimal action-value function}, defined as \\

\begin{equation}
    
q_{*}(s,a) = \max_{\pi} q_{\pi}(s,a) \qquad \text{for all} \: s \in S \: \text{and} \: a \in A(s) 
\end{equation}

% p 67 on 444 q can be written as a function of v

Because the action-value function maximize the expected return $G_t$, a greedy policy maximizing $q_{\pi}(s,a)$ is an optimal policy.Therefore finding the optimal policy is equivalent to find, at each time step, the action $a$ maximizing the action-value. Hence from $(1)$ and $(2)$ we have that

\begin{align}
q_{*}(s,a) & = \max_a\sum_{s'}\sum_{r}p(s',r|s,a) \Bigg [r + \gamma \max_{a'} q_*(s',a') \Bigg ] \nonumber \\
\end{align}

We can observe that the computation of the optimal action-value function $q_*(s,a)$ take into  account the fact that the agent will play optimally in the next time steps since $q_*(s,a)$ depends on $\max_{a'} q_*(s',a')$.\\

If we take the example of the chess, we can expect that if the player make (in expectation) the best possible move at time $t+1$, the action-value at time $t$ will be higher compared to the situation where the player do not make the best move at time $t+1$.\\\\

% PUT ILLUSTRATION of chess game, where one play bad and where one play good, and of the game at time t. eat the queen reward +10

\textbf{Value iteration algorithm}

If $n$ is the number of equations and if the transition probabilities $p(s',r|s,a)$ are known, the problem is reduced to a system of n equations, and an exact solution can be found with dynamic programming methods.\\

!!supress this and put value iteration algo (p83)


Convergence has been proved for the value iteration algorithm,however there is no proof of convergence for reinforcement learning methods.

\textbf{\large On the scope of the reinforcement learning methods}

\textbf{Episodic case and continuous case}

Reinforcement learning problems can be divided between episodic problems, for which a final state exist, and the continuous case, for which there is no final step\footnote{For instance if the task of the agent was to control traffic signal to optimize the traffic, the traffic never stops}. The latter may require a different treatment, however in this thesis we only need to focus on the episodic case.

\textbf{Prediction problem and Control problem}


\text{Off-line policy and On-line policy}



\subsubsection{Tabular methods in reinforcement learning}


In this section we briefly explore methods based on the use of a table, called Q-table, to represent the action-values. Because these methods require to represent all the tuples $(S,A)$, they are not suitable for applications where the action and/or the state space become too large. For this reason we will only expose two algorithms to illustrates two different class of methods : Monte Carlo Methods and Temporal Difference method.\\

\subsection{Monte Carlo methods}


$q_\pi(s,a)$ can be approximated by doing repeated simulations (Making the agent to play chess repeatedly for instance) and keeping a count of the expected return associated with each possible tuple $(a,s)$. As the number of simulation goes to infinity, these averages converge to the action values $q_\pi(s,a)$. Because we estimate an expectation by averaging over many random samples, estimation methods of this kind are called \textit{Monte Carlo Methods}.


However in a lot of cases the number of doubletons $(a,s)$ can be relatively high, making those methods unsuitable.

An alternative approach is to consider $q_\pi(s,a)$ as a parametric function with fewer parameters than states, we will explore these methods in the chapter on value function approximation.

\subsubsubsection{\large Temporal-Difference learning}


\textbf{Q-learning}

While Monte-Carlo methods update the action-value function only at the end of the episode, TD methods update it after each time step. This allows the agent to improve its policy during an episode. For instance if the problem is to learn tennis table, and that our agent makes a bad move $a$ given a state $s$, it leads him to a state associated with low or negative reward, and the action-value of the action of the tuple $(s,a)$ is immediately reduced.

% put a funny tennis image here



\begin{table}
\caption {Methods comparison} \label{tab:title} 
\resizebox{17cm}{!}{\begin{tabular}{|c|c|c|c|}
\hline 
 & \textbf{Dynamic programming } & \textbf{Monte Carlo methods} & \textbf{TD learning}\tabularnewline
\hline 
\hline 
Action-value function updated after\dots{}  & every time step  & every episode & every time step \tabularnewline
\hline 
Require a prior knowledge of the transition probabilities  & Yes & No & No\tabularnewline
\hline 
Suited for continuous problems & Yes & No & Yes\tabularnewline
\hline 

\end{tabular}}
\end{table}









\subsection{Value function approximation}


In a lot of applications, getting a good approximation of the optimal policy and action-value function is computationally infeasible with tabular methods.For instance in chess, they are as many potential states than difference disposition of the game.
Visiting a significant number of times\footnote{Enough times to converge toward the expected value of every possible actions for each possible state} each of those states and trying all the potential actions each time would lead to a ridiculous number of iterations and memory requirements. In 1950, Claude Shannon roughly derived a conservative lower bound for the number of possible combinations in a chess game: $10^{120}$...\footnote{This number is known as the Shannon number.  By comparison, the number of elementary particles is the observable universe is estimated is estimated to be between  $10^80$ and $10^85$.}\\\\

In the chess example, our agent will often be in the situation where it encounters a state that has never been seen before. To choose the right action in these situations, the agent need a capacity of generalization \textit{i.e} to use the information collected from "similar" states previously encountered to make the right decision in a new situation. Fortunately, both statistics and the machine learning communities have extensively studied the problem of extracting general patterns from a finite amount of data.

Instead of representing the approximate action-value function as a table, we now represent it as a function $\hat{q_\pi}(s,a,w)$ where $w$ is a vector of parameters such that $w\in \mathbb{R}^d$ with $d$ \textbf{$d<<|S|$}. As the agent interacts with its environment, the model will be gradually trained with the generated data.
% gnÃ©gnÃ© Because $d<|S|$, the agent will actually %have to infer from the data

The estimator $\hat{q_\pi}$ can be chosen from a wide variety of models, from linear regression to neural networks through splines. In practice, because the vector of parameters $w$ and the set of action-values $(s,a)$ are typically of high dimensions, artificial neural networks have been shown to perform especially well for this task.

\textbf{However problems specific to reinforcement learning arise:}

\begin{enumerate}
    \item \underline{The distribution of the training set can change over time}: In the chess example, % see later just feed the model each time? 
    \item \underline{The target variable change over time}: $q_\pi$ depends on the  policy $\pi$, which improve over time.
\end{enumerate}

 stoch grad desc
standard stochastic approximation conditions sgd gar

speak about application to monte c and td,
show algo for td



\subsection{Policy gradient methods}

The goal of these methods is to directly learn a parameterized policy instead of learning the value-function. The quality of a policy $\pi_\theta$ is denoted $J(\theta)$.We want to learn:

\begin{center}
     $\underset{\theta}{\mathrm{argmax}} \; J(\theta)$
\end{center}

In the episodic case, the quality of a policy is defined as the start value,\textit{i.e} the expectation of the value-function at the initial state $s_0$ given $\pi_\theta$:

%"We can simplify the notation without losing any meaningful generality
%by assuming that every episode starts in some particular (non-random) state s0." SEE THAT LATER!
\begin{center}
$$J(\theta)= \mathop{\mathbb{E_\pi}}[v_{\pi_{\theta}}(S_0)] = \sum_s d(s|\pi_\theta)v_{\pi_{\theta}}(S_0)$$   \end{center}
Where $d(s|\pi_\theta)$ is the stationary distribution of the states: $$\sum_s d(s_t=s|\pi_\theta)= \lim_{t\to\ \infty} p(s|s_0,\pi_\theta)$$

We can observe that the optimal set of parameters $\theta$ maximize the value the agent can expect to receive starting from the state $s_0$ if it follows the policy $\pi_\theta$ during the simulation.\\

The \textit{numerical preferences} of the agent are denoted \item $h(s,a,\theta)$. It can be any continuous and derivable function mapping the $(s,a)$ to a scalar reflecting the preferences of the agent, a neural network for instance. These numerical preferences need to be translated into probabilities to obtain a stochastic policy such that

$$\pi(a|s,\theta) \in (0,1), \text{ for all } s,a,\theta$$\\

This done done via a softmax function, defining the policy as
$$\pi(a|s,\theta)=\frac{exp(\theta^T h(s,a,\theta)}{\sum_b exp(h(s,b,\theta)}$$\\

Where:
    \item $\sum_b$ is the sum over all possible actions in state $s$ .

With a continuous parameterization of the police, the probabilities of each action vary smoothly with respect to $\theta$. In comparison, action-value function approximation methods can cause abrupt changes in the policy.\\ 

For instance, consider a policy for which the action $a*= \max_a \hat{q}_\pi(s,a,w)$ will be chosen by the agent in $(1-\epsilon) \%$ of the time given the state $s$.  A small change in the weight vector $w$ can change the action affected with the highest value in a given state. Since the agent follows a (epsilon) greedy strategy, it will suddenly start to prefer action $a'$ over action $a$.\\
% put neural architecture example here


Thanks to this continuity, gradient policy methods can approximate gradient descent.\\
% benefit for convergence garantees do some research about gradient descent + put reference

To maximize the policy, one possibility is to use a simple stochastic gradient ascent
\begin{center}
\begin{equation}
\theta_{t+1}=\theta_{t}+\alpha \widehat{\nabla J(\theta_t)}
\end{equation}
\end{center}\\


Advantage of softmax: If the optimal policy is deterministic, then
the preferences of the optimal actions will be driven infinitely higher than all suboptimal actions (

put example

\subsubsection{Advantages the policy gradient methods over value-function approximation methods}

1- 
2-

To ensure exploration of the space stocha policy (related to problem of local minima)

 policy may be easier to approximate why?
(as seems to be the case with Tetris; see
Simsek, Algorta, and Kothiyal, 2016).

\begin{table}
\caption {Comparing policy gradients methods and Value-function approximation methods} \label{tab:title} 
\resizebox{17cm}{!}{\begin{tabular}{|c|c|c|}
\hline 
& \textbf{Value-function approximation} & \textbf{Policy Gradient}\tabularnewline
\hline 
\hline 
Action-value function updated after\dots{}  & every time step  & every episode & every time step \tabularnewline
\hline 
Require a prior knowledge of the transition probabilities  & Yes & No & No\tabularnewline
\hline 
Suited for continuous problems & Yes & No & Yes\tabularnewline
\hline 

\end{tabular}}
\end{table} \\

\textbf{Policy Gradient Theorem}\\

Computing $\nabla J(\theta) $ with respect to the policy parameter $\theta$ is not obvious.The effect of $\theta$ on the performance $J$ depends both on the action selection (determined by $\pi_\theta$ and observable) and of the stationary distribution of the states (indirectly determined by $\pi_\theta$ and not observable\footnote{Since the environment is not known.}). In the chess example, if if agent modify its policy, it its move and by thus its reward, but it may also affects the probability to see some game disposition more or less often, depending of  the reaction of its opponent (environment).\\

Providentially, the \textit{gradient policy theorem}\footnote{The proof (Sutton & Barto, 2017; Sec. 13.1)  is not provided here since it is considering its length.} establishes that
\begin{center}
$\nabla_\theta J(\theta) \propto  \sum_s d(s|\pi_\theta) \sum_a q_\pi\nabla_\theta \pi(a|s,\theta)$
\end{center}

Therefore the derivative does not depends anymore of the stationary distribution of the states.\\




\subsubsection{ REINFORCE algorithm}

As we have seen, we optimize $J$ with stochastic gradient ascent so that the updates are:
$$\theta_{t+1}=\theta_{t}+\alpha \widehat{\nabla J(\theta_t)}$$


We only need $\widehat{\nabla J(\theta_t)}$ to be proportional to $\nabla J(\theta)$ since any constant can be adjusted with the step-size $\alpha$.Meanwhile, we need to express $\nabla J(\theta)$ in such as way that we could sample from it.\\

Since is the probability $d(s|\pi_\theta)$ to observe the state $s$ we can write:

\begin{align}
\nabla_\theta J(\theta) & \propto  \sum_s d(s|\pi_\theta) \sum_a q_\pi(s,a)\nabla_\theta \pi(a|s,\theta) \\ & = \mathop{\mathbb{E_\pi}}[\sum_a q_\pi(S_t,a) \nabla_\theta \pi(a|S_t,\theta)]
\end{align}

Similarly, $a$ can be replaced by its sample $A_t$ in the expression.

\begin{align}
\nabla_\theta J(\theta) & = \mathop{\mathbb{E_\pi}}[\sum_a \pi(a |S_t,\theta) q_\pi(S_t,a)
\frac{\nabla_\theta \pi(a|S_t,\theta)}{\pi(a |S_t,\theta)}]\\ & = \mathop{\mathbb{E_\pi}}[ q_\pi(S_t,A_t)
\frac{\nabla_\theta \pi(A_t|S_t,\theta)}{\pi(A_t |S_t,\theta)}]\\ & = \mathop{\mathbb{E_\pi}}[ G_t \frac{\nabla_\theta \pi(A_t|S_t,\theta)}{\pi(A_t |S_t,\theta)}]
\end{align}
Since $q_\pi(s,a) = \mathop{\mathbb{E_\pi}}[G_t | S_t=s,A_t=a]$\\

We can observe that we now have an expression for which we can draw samples.


The stochastic gradient update becomes
$$\theta_{t+1}=\theta_{t}+\alpha \nabla G_t \frac{\nabla_\theta \pi(A_t|S_t,\theta)}{\pi(A_t |S_t,\theta)}$$

The update is proportional to the product of the return $G_t$, by the gradient of the policy of the action took by the agent scaled by the probability of this action.

\begin{itemize}
    \item If the return $G_t$ is low, the update will be small.
    \item The more certain is the agent about the action, the smaller will be the update.\\
\end{itemize} \\

The last point makes sense since the most probable action will tend to be selected, and updated, more often. The term $\frac{1}{\pi(A_t |S_t,\theta)}$ ensure that all the actions receive an equal "amount" of updates independently of their probability to be chosen. For instance if it wasn't the case, an action with low probability but high return may not be updated as much as it should.\\

$\frac{\nabla_\theta \pi(A_t|S_t,\theta)}{\pi(A_t |S_t,\theta)}$ is called \textit{eligibility vector}. In practice, it is rewritten as $\nabla \ln{\pi(A_t |S_t,\theta)}$ using the identity $\nabla ln x =  \frac{\nabla x}{x}$. This is essentially done to simplify the computation of the gradient.\\

Because our algorithm used the return $G_t$ in its updates, which only can be computed at the end of an episode, REINFORCE is a Monte Carlo algorithm, and only update the policy once the entire episode have been completed.\\

\textit{\large {REINFORCE algorithm}}
  \begin{enumerate}
  \item Initialize the policy parameter Î¸ at random.
  \item Generate one trajectory on policy $\pi_\theta$: $S_1, A_1, R_2, S_2, A_2,...,S_T.$
  \item For t=0, 2, â€¦ , T-1:\\
  \indent 1. Estimate the the return  $G_t$\\
  \indent 2. Update policy parameters: $\theta \leftarrow \theta + \alpha \gamma^t G_t \nabla_\theta \ln \pi_\theta(A_t \vert S_t)$
  \end{enumerate}
 
\\
Technically, this is only true if each episode's updates are done o-line, meaning they are accumulated on the side
during the episode and only used to change  by their sum at the episode's end. However, this would probably be a
worse algorithm in practice, and its desirable theoretical properties would probably be shared by the algorithm as given
(although this has not been proved).
\\

Because REINFORCE is a Monte-Carlo algorithm, it suffers from the drawbacks of this class of methods: REINFORCE may performance may have an high variance, and hence learn slowly the optimal policy.


\subsubsection{REINFORCE with baseline}

A common variation of the REINFORCE algorithm is to subtract a baseline $b$ value from the return $G_t$ to reduce the variance of gradient estimation while keeping the bias unchanged:

$$\nabla_\theta J(\theta) & \propto  \sum_s d(s|\pi_\theta) \sum_a (q_\pi(s,a)-b(s)) \nabla_\theta \pi(a|s,\theta) $$\\

This baseline can be any function who do not depend of $a$ so that

$$\sum_a b(s) \nabla_\theta \pi(a|s,\theta) = b(s) \nabla_\theta 1 = 0 $$

Therefore the gradient stays unbiased.

The stochastic gradient update becomes

$$\theta_{t+1}=\theta_{t}+\alpha \nabla (G_t-b(S_t)) \frac{\nabla_\theta \pi(A_t|S_t,\theta)}{\pi(A_t |S_t,\theta)}$$


\underline{Why introducing a baseline can reduce the variance?}\\

Intuitively,the raw value of a trajectory$S_1, A_1, R_2, S_2, A_2,...,S_T.$ is not very meaningful, we are more interested to know whether a reward is higher or smaller than what could be expected. For instance, if the return $G_t$ were always positive (no negative rewards), we would keep updating the probabilities of all action to make them higher. Adding a baseline solves the problem.

A simple example of baseline is to use a moving avergage

PUT Example of moving average used in nas or enas here


\section{Introduction to recurrent neural networks}
%Graves (2012).
Recurrent neural networks (Rumelhart et al., 1986a) are a specific type of neural network designed to model sequential data, so that they can handle much large sequence than casual neural networks.\\

Similarly to convolutional neural networks, recurrent neural networks (RNN) share parameters across different parts of the model; As we will see, the same weights are shared over different time steps.


A sequence contains input vectors $x^(t)$, where $x^(t)$ denotes the vector at step $t$. Depending of the applications, $t$ can represent the time or a step specific to the application.For instance $x^t$ could be the t-th word of a sentence. 


There is a lot of ways to represent a RNN. In fact, any function involving recurrence can be considered a RNN.For this reason we only present the common kind of architectures.

The value of a hidden RNN unit is typically defined as

\begin{align}
h^{(t)}  & = g^{(t)}(x^{(t)},x^{(t-1)},...,x^{(1)})\\
& = f(h^{(t-1)},x^{(t)};\theta)
\end{align}
This relation is illustrated below:

\begin{figure}[H]
\includegraphics[scale=1]{simpleRNN.jpg}
\caption{Two equivalent Illustrations\cite{Goodfellow-et-al-2016} of a RNN.The black square indicate a delay of one step, so that $h_t$ takes $h^{(t-1)}$ in input.In this case each hidden layer produce a prediction $o$ ($\hat{y}$). }
\end{figure}

Where $W$,$U$ and $V$ are weight matrix parametrizing the connections.
We can observe that the value of the th-hidden layer $h^{(t)}$ is both a function the current input $x^t$ and of the past inputs in the form of $h^{(t-1)}$.For instance,in the case of a time series prediction, the model will filter the information coming from the previous layers $h^{(t-2)}$ and select the information relevant for the next hidden layers in order to minimize the prediction error.

We can note that the recursive expression of $h^{(t)}$ allows the model to take sequences of variable length as input. \\

In addition,if we make the hypothesis that the same parameters can be used for different time steps, we can take the same function f for every transition. \textbf{Thank to this hypothesis of shared parameters, we only have to learn one function f.}\\

Similarly to the parameter sharing hypothesis of the convolutional neural network, this hypothesis constrains the model and considerably reduce the number of parameters to learn.\\

\begin{figure}[H]
\includegraphics[scale=0.5]{RNNZOO.jpg}
\caption{The form of a RNN is adapted to its applications. (from Coursera, Andrew Ng)}
\end{figure}

\subsection{One to many recurrent neural network}

In the case of neural architecture search,the model used is a "One to many" RNN as depicted in FIGURE
where x is WHAT and the $\hat{y}$ correspond to the hyperparameters to predict (In the case of a CNN, we predict the type of layer, its number of filter, the size of its filters, etc...).

\begin{figure}[H]
\includegraphics[scale=0.8]{oneToMany.JPG}
\caption{Example of a "One to many" RNN with recurrent connections from the output at one time step to the hidden units at next time step.}
\end{figure}
\\
The model illustrated above is described more formally below:

\begin{align}
    \text{If }t=1:\\
    & a^{(t)} =b +Ux\\
        & h^{(t)} = \sigma_h(a^{(t)})\\
        &  \hat{y}^{(t)}= \sigma_y(c+Vh^{(t)})\\
    \text{If }t>1:\\
        & a^{(t)} =b + W\hat{y}^{(t-1)}\\
        & h^{(t)} = \sigma_h(a^{(t)})\\
        & \hat{y}^{(t)}= \sigma_y(c+Vh^{(t)})
\end{align}

Where the $\sigma$'s represent activation functions, $b$ the bias parameter and $x$ the input vector. \\
For sake of brevity, this type of RNN will be referred to as the "O-H model" ("One to many" RNN with recurrent connections from Output to Hidden units).

The loss function computes the sum of the losses over all the times step:

$$L(x,y^{(1)},...,y^{(T)}) = \sum_t L^{(t)}$$


\subsubsection{Teacher forcing procedure}

One could wonder why the recurrent connections of the O-H model are defined from $y^{(t-1)}$ to $h^{(t)}$ rather than from $h^{(t-1)}$ to $h^{(t)}$.
Indeed, directly connecting $\hat{y}^{(t-1)}$ to $h^{(t)}$ imply that $\hat{y}^{(t-1)}$ plays two roles:
\begin{enumerate}
    \item Estimate $y^{(t-1)}$.
    \item Transmit past information to $h^{(t-1)}$.
\end{enumerate}

However the optimal prediction may require to discard information that is not important at time $t-1$ but that will be valuable in the future. The model depicted in FIGURE set apart the estimation of the features to pass at time $t$ and the estimation of $y^{(t-1)}$ thank to two different weight matrices $W$ and $V$.\\

\begin{figure}[H]
\includegraphics[scale=0.8]{HHmodel.JPG}
\caption{A "One to many" RNN with recurrent connections from the hidden unit at one time step to the hidden unit at next time step.}
\end{figure}
 
\medskip
Yet despite this fact, defining a RNN with recurrent connections between its outputs and its hidden units is preferable in a lot of cases for computational reasons.\\

At training time, the computation of the gradient for the model defined in FIGURE is an expensive operation; The hidden unit $h^{(t-1)}$ depends on all the previous time steps, so that the computation of the gradient has to be sequential, requiring a forward pass through the network --to compute the output of all the hidden units-- before the backward propagation.\footnote{This expensive process is called \textbf{Backpropagation through time}.} 

On the other hand, the O-H model can be trained with \textbf{teacher forcing}.\\
In the O-H model, $$h^{(t)}=\sigma(b + W\hat{y}^{(t-1)}) $$ for $t>1$. Teacher forcing consists to replace $ \hat{y^{(t-1)})}$ by the training set target $y^{(t-1)})$ at training time, so that the gradient for each time step $t$ can be computed independently from the other time steps. For prediction or testing, the target $y$ is not known and is approximated by $\hat{y}$. Teacher forcing allows to backpropagate the error at each time step independently from the others, in a parallel fashion.

\begin{figure}[H]
\includegraphics[scale=0.7]{trainTime.JPG}
\caption{Illustration of the teacher forcing process. During the training, $\hat{y}^{(t-1)}$ is replaced by $y^{(t-1)}$ in the backpropagation equations.}
\end{figure}



\subsubsection{Backpropagation through time}

Backpropagation through time (BPTT) is an adaptation of the classic backpropagation algorithm. In a casual neural network, the weights are typically different for every layer. In a RNN, U, V and W are shared of all the time steps, hence the gradient of the loss function $L$ w.r.t to a weight matrix (say $U$) is the sum of the gradients at every time steps:


$$ \frac{\partial L}{\partial U}=\sum_t \frac{\partial L_t}{\partial U} $$

Except for this fact, BPTT is virtually identical to the casual propagation, using the chain rule to unroll the computation of the gradient.For instance in the case of the RNN depicted in FIGURE, the gradients w.r.t ... can be computed as follows:

$$  \frac{\partial L}{\partial U}=\sum_t \frac{\partial L_t}{\partial U} $$

just write mathematical theoric formula 

why tanh and softmax?
In the 1990s, important progress have been made in the modelisation of sequences with neural networks.

Hochreiter and Schmidhuber (1997) proposed the long short-term memory (LSTM) network to adress some of these difficulties.

\section{Neural architecture search with reinforcement learning}

In this section, we explore several recent papers which have showed impressive results using reinforcement learning techniques to automatically design the architecture of a neural network.





\chapter{Neural Architecture Search with Network Morphism}

\chapter{my approach}




\end{document}


